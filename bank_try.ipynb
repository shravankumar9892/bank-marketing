{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import boto3\n",
    "import botocore\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset : downloading - preprocessing - uploading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the [dataset](http://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip) and keep it in the data folder with name 'bankadditionalfull.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('data/bankadditionalfull.csv', sep=';', index_col=0)\n",
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_categorical(dataframe):   # Finds categorical data from the dataframe\n",
    "    total = dataframe.columns\n",
    "    numerical = dataframe._get_numeric_data().columns\n",
    "    dictionary = {'CATEGORICAL': list(set(total) - set(numerical)), 'NUMERIC':list(numerical)}\n",
    "    return dictionary\n",
    "\n",
    "features = identify_categorical(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not execute this code block! It is only shown for understanding purpose\n",
    "# As Amazon ML accepts objects type of data as well as long as we specify data types correctly\n",
    "\n",
    "for columns in features['CATEGORICAL']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(raw_data[columns])\n",
    "    raw_data[columns] = le.transform(raw_data[columns])\n",
    "    del(le)\n",
    "\n",
    "# Saving the file\n",
    "# This file needs to be saved in S3 Bucket to use Amazon ML\n",
    "raw_data.to_csv('data/bankadditionalfull_.csv', index=False)\n",
    "\n",
    "raw_data.head(2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting off with using amazon services, do [this](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html)\n",
    "\n",
    "And it is mandatory to save the data in either S3 or RedShift, otherwise you cannot use Amazon ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If data already exists in S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if a bucket exists already \n",
    "s3 = boto3.client('s3')\n",
    "response = s3.list_buckets()\n",
    "\n",
    "# response is a dictionary, which gives\n",
    "# metadata, bucket name & owner Id\n",
    "bucket = [buckets['Name'] for buckets in response['Buckets']]\n",
    "BUCKET_NAME = bucket[0]\n",
    "KEY = 'bankadditionalfull_.csv' # File name in the bucket, not the Owner ID.\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "try:\n",
    "    s3.Bucket(BUCKET_NAME).download_file('bankadditionalfull_.csv', 'data/bankadditionalfull_.csv')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == \"404\":\n",
    "        print(\"The file does not exist.\")\n",
    "    else:\n",
    "        raise        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If data is in your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For users who do not have a S3 Bucket created\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.create_bucket(Bucket='<your_bucket_name>')\n",
    "s3.upload_file(filename, '<your_bucket_name>', filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To know more about using boto3 to access S3 buckets click [here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-creating-buckets.html)\n",
    "\n",
    "- Since we have saved our dataset in the S3 Bucket, we can now move forward to creating ML model.\n",
    "\n",
    "- First we need to create a datasource. A datasource is basically the information of our dataset. Like, \n",
    "    * Where is it stored\n",
    "    * Info of the data features (aka categorical/numerical/text/binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DataSource](images/createdatasource.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('machinelearning', endpoint_url='https://machinelearning.us-east-1.amazonaws.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating JSON file for DataSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied this from boto3 documentation\n",
    "# Even you copy it as it is\n",
    "DataSchema = { \n",
    "    \"version\": \"1.0\",\n",
    "    \"targetFieldName\": \"y\",\n",
    "    \"dataFormat\": \"CSV\",\n",
    "    \"dataFileContainsHeader\": 'true', # Set it to true because, CSV contains feature names.\n",
    "    }\n",
    "\n",
    "# Now we will fill the \"attributes\"\n",
    "attributes = []\n",
    "for featureType in list(features.keys()):\n",
    "    for featureName in features[featureType]:\n",
    "        attributes.append({'fieldName':featureName, 'fieldType':featureType})\n",
    "        \n",
    "DataSchema['attributes'] = attributes  \n",
    "\n",
    "# Saving DataSchema in a JSON file\n",
    "with open('data/dataschema.json', 'w') as outfile:\n",
    "    json.dump(DataSchema, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure for Amazon ML you set your region name to 'us-east-1' or 'eu-west-1'\n",
    "# As AML works only for US East(Virginia) and EU (Ireland) as of now.\n",
    "\n",
    "response = client.create_data_source_from_s3(\n",
    "    DataSourceId='string',\n",
    "    DataSourceName='Example Predictions',  # Any name will do\n",
    "    DataSpec={\n",
    "        'DataLocationS3': 's3://bankclassification/bankadditionalfull_.csv', # s3://bucket_name/file_name\n",
    "        'DataSchema': 's3://bankclassification/dataschema.json'\n",
    "    },\n",
    "    ComputeStatistics=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
